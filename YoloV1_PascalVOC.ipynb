{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-06T14:34:01.725732Z","iopub.execute_input":"2025-03-06T14:34:01.726092Z","iopub.status.idle":"2025-03-06T14:34:16.407145Z","shell.execute_reply.started":"2025-03-06T14:34:01.726066Z","shell.execute_reply":"2025-03-06T14:34:16.406185Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Téléchargement + extraction PascalVOC","metadata":{}},{"cell_type":"code","source":"import os\nimport tarfile\nimport urllib.request\n\ndef download_and_extract(url, output_dir):\n    filename = url.split(\"/\")[-1]\n    \n    print(f\"Téléchargement de {filename}...\")\n    urllib.request.urlretrieve(url, filename)\n    print(f\"Extraction de {filename}...\")\n    with tarfile.open(filename, \"r\") as tar:\n        tar.extractall(path=output_dir)\n    os.remove(filename)\n    print(f\"{filename} supprimé.\")\n\nif __name__ == \"__main__\":\n    urls = [\n        \"http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\",\n        \"http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\"\n    ]\n    output_dir = \"./VOC_dataset\"  # Dossier où extraire les fichiers\n    os.makedirs(output_dir, exist_ok=True)\n    \n    for url in urls:\n        download_and_extract(url, output_dir)\n    \n    print(\"Téléchargement et extraction terminés.\")\n\nimport requests\nimport tarfile\nimport os\n\n# URLs for the datasets\ntrainval_url = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\"\ntest_url = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\"\n\n# Function to download a file\ndef download_file(url, filename):\n    response = requests.get(url)\n    with open(filename, 'wb') as f:\n        f.write(response.content)\n\n# Download the files\ndownload_file(trainval_url, \"VOCtrainval_06-Nov-2007.tar\")\ndownload_file(test_url, \"VOCtest_06-Nov-2007.tar\")\n\n# Extract the tar files\nwith tarfile.open(\"VOCtrainval_06-Nov-2007.tar\") as tar:\n    tar.extractall()\nwith tarfile.open(\"VOCtest_06-Nov-2007.tar\") as tar:\n    tar.extractall()\n\n# Remove the tar files\nos.remove(\"VOCtrainval_06-Nov-2007.tar\")\nos.remove(\"VOCtest_06-Nov-2007.tar\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-06T14:34:16.408467Z","iopub.execute_input":"2025-03-06T14:34:16.408992Z","iopub.status.idle":"2025-03-06T14:36:04.531312Z","shell.execute_reply.started":"2025-03-06T14:34:16.408967Z","shell.execute_reply":"2025-03-06T14:36:04.530396Z"}},"outputs":[{"name":"stdout","text":"Téléchargement de VOCtrainval_06-Nov-2007.tar...\nExtraction de VOCtrainval_06-Nov-2007.tar...\nVOCtrainval_06-Nov-2007.tar supprimé.\nTéléchargement de VOCtest_06-Nov-2007.tar...\nExtraction de VOCtest_06-Nov-2007.tar...\nVOCtest_06-Nov-2007.tar supprimé.\nTéléchargement et extraction terminés.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Annotations","metadata":{}},{"cell_type":"code","source":"import argparse\nimport xml.etree.ElementTree as ET\nimport os\n\nparser = argparse.ArgumentParser(description='Build Annotations.')\nparser.add_argument('dir', default='..', help='Annotations.')\n\nsets = [('2007', 'train'), ('2007', 'val'), ('2007', 'test')]\n\nclasses_num = {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4, 'bus': 5,\n               'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10, 'dog': 11,\n               'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15, 'sheep': 16,\n               'sofa': 17, 'train': 18, 'tvmonitor': 19}\n\n\ndef convert_annotation(year, image_id, f): #Extraction des coordonés nécéssaire pour YOLO depuis les XML\n    in_file = os.path.join('VOCdevkit/VOC%s/Annotations/%s.xml' % (year, image_id))\n    tree = ET.parse(in_file)\n    root = tree.getroot()\n\n    for obj in root.iter('object'):\n        difficult = obj.find('difficult').text\n        cls = obj.find('name').text\n        classes = list(classes_num.keys())\n        if cls not in classes or int(difficult) == 1:\n            continue\n        cls_id = classes.index(cls)\n        xmlbox = obj.find('bndbox')\n        b = (int(xmlbox.find('xmin').text), int(xmlbox.find('ymin').text),\n             int(xmlbox.find('xmax').text), int(xmlbox.find('ymax').text))\n        f.write(' ' + ','.join([str(a) for a in b]) + ',' + str(cls_id))\n\nfor year, image_set in sets:\n  print(year, image_set)\n  with open(os.path.join('VOCdevkit/VOC%s/ImageSets/Main/%s.txt' % (year, image_set)), 'r') as f:\n      image_ids = f.read().strip().split()\n  with open(os.path.join(\"VOCdevkit\", '%s_%s.txt' % (year, image_set)), 'w') as f:\n      for image_id in image_ids:\n          f.write('%s/VOC%s/JPEGImages/%s.jpg' % (\"VOCdevkit\", year, image_id))\n          convert_annotation(year, image_id, f)\n          f.write('\\n')\n\nimport cv2 as cv\nimport numpy as np\n\ndef read(image_path, label, taillegris=7):\n    image = cv.imread(image_path)\n    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n    image_h, image_w = image.shape[0:2]\n    image = cv.resize(image, (448, 448))\n    image = image / 255.\n\n    label_matrix = np.zeros([taillegris, taillegris, 30])\n    for l in label:\n        l = l.split(',')\n        l = np.array(l, dtype=np.int32)\n        xmin = l[0]\n        ymin = l[1]\n        xmax = l[2]\n        ymax = l[3]\n        cls = l[4]\n        x = (xmin + xmax) / 2 / image_w\n        y = (ymin + ymax) / 2 / image_h\n        w = (xmax - xmin) / image_w\n        h = (ymax - ymin) / image_h\n        loc = [taillegris * x, taillegris * y]\n        loc_i = int(loc[1])\n        loc_j = int(loc[0])\n        y = loc[1] - loc_i\n        x = loc[0] - loc_j\n\n        if label_matrix[loc_i, loc_j, 24] == 0:\n            label_matrix[loc_i, loc_j, cls] = 1\n            label_matrix[loc_i, loc_j, 20:24] = [x, y, w, h]\n            label_matrix[loc_i, loc_j, 24] = 1 \n\n    return image, label_matrix\n\nimage_path = 'VOCdevkit/VOC2007/JPEGImages/000002.jpg'\nlabel = [\"139,200,207,301,18\"]\nim , mat = read(image_path, label)\nmat","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-06T14:36:04.532498Z","iopub.execute_input":"2025-03-06T14:36:04.532750Z","iopub.status.idle":"2025-03-06T14:36:05.952350Z","shell.execute_reply.started":"2025-03-06T14:36:04.532729Z","shell.execute_reply":"2025-03-06T14:36:05.951304Z"}},"outputs":[{"name":"stdout","text":"2007 train\n2007 val\n2007 test\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"array([[[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       ...,\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]]])"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Génération des set train/val batcher","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\n\nclass My_Custom_Generator(keras.utils.Sequence) :\n\n  def __init__(self, images, labels, batch_size) :\n    self.images = images\n    self.labels = labels\n    self.batch_size = batch_size\n\n\n  def __len__(self) :\n    return int((np.ceil(len(self.images) / float(self.batch_size))))\n\n\n  def __getitem__(self, idx) :\n    batch_x = self.images[idx * self.batch_size : (idx+1) * self.batch_size]\n    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n\n    train_image = []\n    train_label = []\n\n    for i in range(0, len(batch_x)):\n      img_path = batch_x[i]\n      label = batch_y[i]\n      image, label_matrix = read(img_path, label)\n      train_image.append(image)\n      train_label.append(label_matrix)\n    return np.array(train_image), np.array(train_label)\n\ntrain_datasets = []\nval_datasets = []\n\nwith open(os.path.join(\"VOCdevkit\", '2007_train.txt'), 'r') as f:\n    train_datasets = train_datasets + f.readlines()\nwith open(os.path.join(\"VOCdevkit\", '2007_val.txt'), 'r') as f:\n    val_datasets = val_datasets + f.readlines()\n\nX_train = []\nY_train = []\n\nX_val = []\nY_val = []\n\nfor item in train_datasets:\n  item = item.replace(\"\\n\", \"\").split(\" \")\n  X_train.append(item[0])\n  arr = []\n  for i in range(1, len(item)):\n    arr.append(item[i])\n  Y_train.append(arr)\n\nfor item in val_datasets:\n  item = item.replace(\"\\n\", \"\").split(\" \")\n  X_val.append(item[0])\n  arr = []\n  for i in range(1, len(item)):\n    arr.append(item[i])\n  Y_val.append(arr)\n\nbatch_size = 4\nmy_training_batch_generator = My_Custom_Generator(X_train, Y_train, batch_size)\n\nmy_validation_batch_generator = My_Custom_Generator(X_val, Y_val, batch_size)\n\nx_train, y_train = my_training_batch_generator.__getitem__(0)\nx_val, y_val = my_training_batch_generator.__getitem__(0)\nprint(x_train.shape)\nprint(y_train.shape)\n\nprint(x_val.shape)\nprint(y_val.shape)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-06T13:26:24.958269Z","iopub.execute_input":"2025-03-06T13:26:24.958579Z","execution_failed":"2025-03-06T14:28:41.430Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Création de la derniere couche du FC, qui retourne la prédiction + proba","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\nclass Yolo_Reshape(tf.keras.layers.Layer):\n    def __init__(self, target_shape):\n        super(Yolo_Reshape, self).__init__()\n        self.target_shape = tuple(target_shape)\n\n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'target_shape': self.target_shape\n        })\n        return config\n\n    def call(self, inputs):\n        # grids 7x7\n        S = [self.target_shape[0], self.target_shape[1]]\n        # classes\n        C = 20\n        # no of bounding boxes per grid\n        B = 2\n\n        idx1 = S[0] * S[1] * C\n        idx2 = idx1 + S[0] * S[1] * B\n\n        # Probabilité des classes \n        class_probs = tf.reshape(inputs[:, :idx1], (tf.shape(inputs)[0], S[0], S[1], C))\n        class_probs = tf.nn.softmax(class_probs)\n\n        # Score de cofidance pour la classe prédite, cette partie ne seras pas utile pour plantdoc\n        confs = tf.reshape(inputs[:, idx1:idx2], (tf.shape(inputs)[0], S[0], S[1], B))\n        confs = tf.sigmoid(confs)\n\n        # Bouding boxe associé\n        boxes = tf.reshape(inputs[:, idx2:], (tf.shape(inputs)[0], S[0], S[1], B * 4))\n        boxes = tf.sigmoid(boxes)\n\n        # Concatenation des 3 ensemble\n        outputs = tf.concat([class_probs, confs, boxes], axis=-1)\n        return outputs","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-06T13:26:24.958269Z","iopub.execute_input":"2025-03-06T13:26:24.958579Z","execution_failed":"2025-03-06T14:28:41.430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, InputLayer, Dropout, Flatten, Reshape\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras import layers, models\n\nlrelu = tf.keras.layers.LeakyReLU(alpha=0.1)\n\nnb_boxes=1 #Etrangement une seule anchors boxes alors que sortie (7,7,30)?\ngrid_w=7 \ngrid_h=7\ncell_w=64\ncell_h=64\nimg_w=grid_w*cell_w\nimg_h=grid_h*cell_h\n\nmodel = Sequential() #Instanciation d'un modele sequentiel depuis tensorflow (module de classe, modele vierge)\nmodel.add(layers.Input(shape=(448, 448, 3))) #Couche d'entré, avec la taille de l'image\nmodel.add(Conv2D(filters=64, kernel_size= (7, 7), strides=(1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) #Première convolution avec 64 filtres de taille 7x7, ici on cherche les gros pattern sur l'image taille initiale\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same')) #Premire max pooling, avec une stride de 2x2, on passe donc à la deuxieme couche de yoloV1, l'image est donc a la taille 112x112\n\nmodel.add(Conv2D(filters=192, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) #Deuxieme convolution, on cherche donc des pattern plus fins avec 192 filtres de taille 3x3\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same')) #On redivise l'image de moitié, vers la 3éme couche du modèle, donc l'image seras la taille 56x56\n\n#On cherche des patterns encore plus fin avec 2 taille de filtre diffrent 3x3 et 1x1\nmodel.add(Conv2D(filters=128, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) #128 filtres de 1x1 \nmodel.add(Conv2D(filters=256, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) #256 filtres de 3x3\nmodel.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) #256 Filtres de 1x1\nmodel.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) #512 Filtres de 1x1\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same')) #On redivise l'image de moitié, vers la 4éme couche du modèle, donc l'image seras la taille 28x28\n\n#On cherche des patterns encore plus fin... on arrive donc a la couche du modèle qui repètre 4x un pattern de filtres + un bloc de 1x1 et un bloc de 3x3\n#BLOC1 (de la répétition x4)\nmodel.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) # 256 Filtres de 1x1    \nmodel.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) # 512 Filtres de 3x3\n#BLOC 2\nmodel.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) \nmodel.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n#BLOC 3\nmodel.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\nmodel.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n#BLOC 4\nmodel.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\nmodel.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n#Et nos 512 et 1024 filtres finaux\nmodel.add(Conv2D(filters=512, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) #512 Filtres de 1x1\nmodel.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) #1024 Filtres de 3x3\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same')) #On redivise l'image de moitié, vers la 5éme couche du modèle, donc l'image seras la taille 14x14\n\n#Pour l'avant derniere couche, on repète un pattern de filtre 2x pour finir avec un bloc de 1024 filtres\n#BLOC 1 (de la répétition x2)\nmodel.add(Conv2D(filters=512, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\nmodel.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n#BLOC 2\nmodel.add(Conv2D(filters=512, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\nmodel.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n\nmodel.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) #Le petit dernier bloc de 1024 Filtres de 3x3\nmodel.add(Conv2D(filters=1024, kernel_size= (3, 3), strides=(2, 2), padding = 'same'))  #On réalise l'équivalent d'un Max Pooling avec une convolution sans activation, on redivise donc l'image pour atteindre la taille de 7x7 (le fameux 7x7 de YOLO).\n\n#Et enfin la derniere convolution du modèle \nmodel.add(Conv2D(filters=1024, kernel_size= (3, 3), activation=lrelu, kernel_regularizer=l2(5e-4))) #1024 Filtres de 3x3\nmodel.add(Conv2D(filters=1024, kernel_size= (3, 3), activation=lrelu, kernel_regularizer=l2(5e-4))) #1024 Filtres de 3x3\n\n#LE FULLY CONNECTED !\nmodel.add(Flatten()) #La couche de flatten qui permet de passer 2D > 1D\nmodel.add(Dense(512)) \nmodel.add(Dense(1024))\nmodel.add(Dropout(0.5)) #Dropout de la moitié des neuronnes pour éviter l'overfitting..\nmodel.add(Dense(1470, activation='sigmoid')) # 1470 car 7*7*30 > pour nous on aura 7*7*5=245 (Je ne sais pas pourquoi dans plantdoc on avais utilisé 7x7x6 alors qu'ici on avais indiqué 7x7x5 pendant les cours...)\nmodel.add(Yolo_Reshape(target_shape=(7,7,30))) #Le reshape de yolo pour extraire les informations pertinentes.\nmodel.summary()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-06T13:26:24.958269Z","iopub.execute_input":"2025-03-06T13:26:24.958579Z","execution_failed":"2025-03-06T14:28:41.430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"# Define a custom learning rate scheduler\n\n\"\"\"\n\nfrom tensorflow import keras\n\nclass CustomLearningRateScheduler(keras.callbacks.Callback):\n    \"\"\"Learning rate scheduler which sets the learning rate according to schedule.\n\n  Arguments:\n      schedule: a function that takes an epoch index\n          (integer, indexed from 0) and current learning rate\n          as inputs and returns a new learning rate as output (float).\n  \"\"\"\n\n    def __init__(self, schedule):\n        super(CustomLearningRateScheduler, self).__init__()\n        self.schedule = schedule\n\n    def on_epoch_begin(self, epoch, logs=None):\n        if not hasattr(self.model.optimizer, \"learning_rate\"):\n            raise ValueError('Optimizer must have a \"learning_rate\" attribute.')\n        # Get the current learning rate from model's optimizer.\n        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n        # Call schedule function to get the scheduled learning rate.\n        new_lr = self.schedule(epoch, lr)\n        # Set the value back to the optimizer before this epoch starts\n        self.model.optimizer.learning_rate.assign(new_lr)\n        print(\"\\nEpoch %05d: Learning rate is %6.4f.\" % (epoch, new_lr))\n\n\nLR_SCHEDULE = [\n    # (epoch to start, learning rate) tuples\n    (0, 0.01),\n    (75, 0.001),\n    (105, 0.0001),\n]\n\n\ndef lr_schedule(epoch, lr):\n    \"\"\"Helper function to retrieve the scheduled learning rate based on epoch.\"\"\"\n    if epoch < LR_SCHEDULE[0][0] or epoch > LR_SCHEDULE[-1][0]:\n        return lr\n    for i in range(len(LR_SCHEDULE)):\n        if epoch == LR_SCHEDULE[i][0]:\n            return LR_SCHEDULE[i][1]\n    return lr","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-06T13:26:24.958269Z","iopub.execute_input":"2025-03-06T13:26:24.958579Z","execution_failed":"2025-03-06T14:28:41.430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"# Define the loss function\n\n\"\"\"\n\nfrom tensorflow.keras import backend as K\n\n\ndef xywh2minmax(xy, wh):\n    xy_min = xy - wh / 2\n    xy_max = xy + wh / 2\n\n    return xy_min, xy_max\n\n\ndef iou(pred_mins, pred_maxes, true_mins, true_maxes):\n    intersect_mins = K.maximum(pred_mins, true_mins)\n    intersect_maxes = K.minimum(pred_maxes, true_maxes)\n    intersect_wh = K.maximum(intersect_maxes - intersect_mins, 0.)\n    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n\n    pred_wh = pred_maxes - pred_mins\n    true_wh = true_maxes - true_mins\n    pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n    true_areas = true_wh[..., 0] * true_wh[..., 1]\n\n    union_areas = pred_areas + true_areas - intersect_areas\n    iou_scores = intersect_areas / union_areas\n\n    return iou_scores\n\n\ndef yolo_head(feats):\n    # Dynamic implementation of conv dims for fully convolutional model.\n    conv_dims = K.shape(feats)[1:3]  # assuming channels last\n    # In YOLO the height index is the inner most iteration.\n    conv_height_index = K.arange(0, stop=conv_dims[0])\n    conv_width_index = K.arange(0, stop=conv_dims[1])\n    conv_height_index = K.tile(conv_height_index, [conv_dims[1]])\n\n    # TODO: Repeat_elements and tf.split doesn't support dynamic splits.\n    # conv_width_index = K.repeat_elements(conv_width_index, conv_dims[1], axis=0)\n    conv_width_index = K.tile(\n        K.expand_dims(conv_width_index, 0), [conv_dims[0], 1])\n    conv_width_index = K.flatten(K.transpose(conv_width_index))\n    conv_index = K.transpose(K.stack([conv_height_index, conv_width_index]))\n    conv_index = K.reshape(conv_index, [1, conv_dims[0], conv_dims[1], 1, 2])\n    conv_index = K.cast(conv_index, K.dtype(feats))\n\n    conv_dims = K.cast(K.reshape(conv_dims, [1, 1, 1, 1, 2]), K.dtype(feats))\n\n    box_xy = (feats[..., :2] + conv_index) / conv_dims * 448\n    box_wh = feats[..., 2:4] * 448\n\n    return box_xy, box_wh\n\n\ndef yolo_loss(y_true, y_pred):\n    label_class = y_true[..., :20]  # ? * 7 * 7 * 20\n    label_box = y_true[..., 20:24]  # ? * 7 * 7 * 4\n    response_mask = y_true[..., 24]  # ? * 7 * 7\n    response_mask = K.expand_dims(response_mask, axis=-1)  # ? * 7 * 7 * 1\n\n    predict_class = y_pred[..., :20]  # ? * 7 * 7 * 20\n    predict_trust = y_pred[..., 20:22]  # ? * 7 * 7 * 2\n    predict_box = y_pred[..., 22:]  # ? * 7 * 7 * 8\n\n    _label_box = K.reshape(label_box, [-1, 7, 7, 1, 4])\n    _predict_box = K.reshape(predict_box, [-1, 7, 7, 2, 4])\n\n    label_xy, label_wh = yolo_head(_label_box)  # ? * 7 * 7 * 1 * 2, ? * 7 * 7 * 1 * 2\n    label_xy = K.expand_dims(label_xy, 3)  # ? * 7 * 7 * 1 * 1 * 2\n    label_wh = K.expand_dims(label_wh, 3)  # ? * 7 * 7 * 1 * 1 * 2\n    label_xy_min, label_xy_max = xywh2minmax(label_xy, label_wh)  # ? * 7 * 7 * 1 * 1 * 2, ? * 7 * 7 * 1 * 1 * 2\n\n    predict_xy, predict_wh = yolo_head(_predict_box)  # ? * 7 * 7 * 2 * 2, ? * 7 * 7 * 2 * 2\n    predict_xy = K.expand_dims(predict_xy, 4)  # ? * 7 * 7 * 2 * 1 * 2\n    predict_wh = K.expand_dims(predict_wh, 4)  # ? * 7 * 7 * 2 * 1 * 2\n    predict_xy_min, predict_xy_max = xywh2minmax(predict_xy, predict_wh)  # ? * 7 * 7 * 2 * 1 * 2, ? * 7 * 7 * 2 * 1 * 2\n\n    iou_scores = iou(predict_xy_min, predict_xy_max, label_xy_min, label_xy_max)  # ? * 7 * 7 * 2 * 1\n    best_ious = K.max(iou_scores, axis=4)  # ? * 7 * 7 * 2\n    best_box = K.max(best_ious, axis=3, keepdims=True)  # ? * 7 * 7 * 1\n\n    box_mask = K.cast(best_ious >= best_box, K.dtype(best_ious))  # ? * 7 * 7 * 2\n\n    no_object_loss = 0.5 * (1 - box_mask * response_mask) * K.square(0 - predict_trust)\n    object_loss = box_mask * response_mask * K.square(1 - predict_trust)\n    confidence_loss = no_object_loss + object_loss\n    confidence_loss = K.sum(confidence_loss)\n\n    class_loss = response_mask * K.square(label_class - predict_class)\n    class_loss = K.sum(class_loss)\n\n    _label_box = K.reshape(label_box, [-1, 7, 7, 1, 4])\n    _predict_box = K.reshape(predict_box, [-1, 7, 7, 2, 4])\n\n    label_xy, label_wh = yolo_head(_label_box)  # ? * 7 * 7 * 1 * 2, ? * 7 * 7 * 1 * 2\n    predict_xy, predict_wh = yolo_head(_predict_box)  # ? * 7 * 7 * 2 * 2, ? * 7 * 7 * 2 * 2\n\n    box_mask = K.expand_dims(box_mask)\n    response_mask = K.expand_dims(response_mask, axis=-1)\n\n    box_loss = 5 * box_mask * response_mask * K.square((label_xy - predict_xy) / 448)\n    box_loss += 5 * box_mask * response_mask * K.square((K.sqrt(label_wh) - K.sqrt(predict_wh)) / 448)\n    box_loss = K.sum(box_loss)\n\n    loss = confidence_loss + class_loss + box_loss\n\n    return loss","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-06T13:26:24.958269Z","iopub.execute_input":"2025-03-06T13:26:24.958579Z","execution_failed":"2025-03-06T14:28:41.430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"# Add a callback for saving the weights\n\n\"\"\"\n\n# defining a function to save the weights of best model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nmcp_save = ModelCheckpoint('weight.keras', save_best_only=True, monitor='val_loss', mode='min')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-06T13:26:24.958269Z","iopub.execute_input":"2025-03-06T13:26:24.958579Z","execution_failed":"2025-03-06T14:28:41.430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"# Compile the model\n\n\"\"\"\n\nfrom tensorflow import keras\n\nmodel.compile(loss=yolo_loss ,optimizer='adam')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-06T13:26:24.958269Z","iopub.execute_input":"2025-03-06T13:26:24.958579Z","execution_failed":"2025-03-06T14:28:41.430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"# Train the model\n\n\"\"\"\n\nmodel.fit(x=my_training_batch_generator,\n          steps_per_epoch = int(len(X_train) // batch_size),\n          epochs = 135,\n          verbose = 1,\n          validation_data = my_validation_batch_generator,\n          validation_steps = int(len(X_val) // batch_size),\n           callbacks=[\n              CustomLearningRateScheduler(lr_schedule),\n              mcp_save\n          ])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}