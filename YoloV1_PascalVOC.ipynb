{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-06T14:34:01.726092Z",
     "iopub.status.busy": "2025-03-06T14:34:01.725732Z",
     "iopub.status.idle": "2025-03-06T14:34:16.407145Z",
     "shell.execute_reply": "2025-03-06T14:34:16.406185Z",
     "shell.execute_reply.started": "2025-03-06T14:34:01.726066Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Téléchargement + extraction PascalVOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-06T14:34:16.408992Z",
     "iopub.status.busy": "2025-03-06T14:34:16.408467Z",
     "iopub.status.idle": "2025-03-06T14:36:04.531312Z",
     "shell.execute_reply": "2025-03-06T14:36:04.530396Z",
     "shell.execute_reply.started": "2025-03-06T14:34:16.408967Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement de VOCtrainval_06-Nov-2007.tar...\n",
      "Extraction de VOCtrainval_06-Nov-2007.tar...\n",
      "VOCtrainval_06-Nov-2007.tar supprimé.\n",
      "Téléchargement de VOCtest_06-Nov-2007.tar...\n",
      "Extraction de VOCtest_06-Nov-2007.tar...\n",
      "VOCtest_06-Nov-2007.tar supprimé.\n",
      "Téléchargement et extraction terminés.\n"
     ]
    }
   ],
   "source": [
    "#Nous avons réalisé l'entrainement en local, donc nous avons du modifier les !wget afin de pouvoir télécharger les datasets dans un fichier .py\n",
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "def download_and_extract(url, output_dir):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    \n",
    "    print(f\"Téléchargement de {filename}...\")\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    print(f\"Extraction de {filename}...\")\n",
    "    with tarfile.open(filename, \"r\") as tar:\n",
    "        tar.extractall(path=output_dir)\n",
    "    os.remove(filename)\n",
    "    print(f\"{filename} supprimé.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    urls = [\n",
    "        \"http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\",\n",
    "        \"http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\"\n",
    "    ]\n",
    "    output_dir = \"./VOC_dataset\"  # Dossier où extraire les fichiers\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for url in urls:\n",
    "        download_and_extract(url, output_dir)\n",
    "    \n",
    "    print(\"Téléchargement et extraction terminés.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-06T14:36:04.532750Z",
     "iopub.status.busy": "2025-03-06T14:36:04.532498Z",
     "iopub.status.idle": "2025-03-06T14:36:05.952350Z",
     "shell.execute_reply": "2025-03-06T14:36:05.951304Z",
     "shell.execute_reply.started": "2025-03-06T14:36:04.532729Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007 train\n",
      "2007 val\n",
      "2007 test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Build Annotations.')\n",
    "parser.add_argument('dir', default='..', help='Annotations.')\n",
    "\n",
    "sets = [('2007', 'train'), ('2007', 'val'), ('2007', 'test')]\n",
    "\n",
    "classes_num = {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4, 'bus': 5,\n",
    "               'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10, 'dog': 11,\n",
    "               'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15, 'sheep': 16,\n",
    "               'sofa': 17, 'train': 18, 'tvmonitor': 19}\n",
    "\n",
    "\n",
    "def convert_annotation(year, image_id, f): #Extraction des coordonés nécéssaire pour YOLO depuis les XML\n",
    "    in_file = os.path.join('VOCdevkit/VOC%s/Annotations/%s.xml' % (year, image_id))\n",
    "    tree = ET.parse(in_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    for obj in root.iter('object'):\n",
    "        difficult = obj.find('difficult').text\n",
    "        cls = obj.find('name').text\n",
    "        classes = list(classes_num.keys())\n",
    "        if cls not in classes or int(difficult) == 1:\n",
    "            continue\n",
    "        cls_id = classes.index(cls)\n",
    "        xmlbox = obj.find('bndbox')\n",
    "        b = (int(xmlbox.find('xmin').text), int(xmlbox.find('ymin').text),\n",
    "             int(xmlbox.find('xmax').text), int(xmlbox.find('ymax').text))\n",
    "        f.write(' ' + ','.join([str(a) for a in b]) + ',' + str(cls_id))\n",
    "\n",
    "for year, image_set in sets:\n",
    "  print(year, image_set)\n",
    "  with open(os.path.join('VOCdevkit/VOC%s/ImageSets/Main/%s.txt' % (year, image_set)), 'r') as f:\n",
    "      image_ids = f.read().strip().split()\n",
    "  with open(os.path.join(\"VOCdevkit\", '%s_%s.txt' % (year, image_set)), 'w') as f:\n",
    "      for image_id in image_ids:\n",
    "          f.write('%s/VOC%s/JPEGImages/%s.jpg' % (\"VOCdevkit\", year, image_id))\n",
    "          convert_annotation(year, image_id, f)\n",
    "          f.write('\\n')\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "def read(image_path, label, taillegris=7):\n",
    "    image = cv.imread(image_path) # Lecture de l'image\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB) # Conversion format OpenCV vers RGB\n",
    "    #Resize et normalisation\n",
    "    image_h, image_w = image.shape[0:2] \n",
    "    image = cv.resize(image, (448, 448))\n",
    "    image = image / 255.\n",
    "\n",
    "    label_matrix = np.zeros([taillegris, taillegris, 30]) # Construction de la vérité terrain au même format que Yolo (7,7,30) pour PascalVoc\n",
    "    for l in label:\n",
    "        l = l.split(',')\n",
    "        l = np.array(l, dtype=np.int32)\n",
    "        xmin = l[0]\n",
    "        ymin = l[1]\n",
    "        xmax = l[2]\n",
    "        ymax = l[3]\n",
    "        cls = l[4]\n",
    "        x = (xmin + xmax) / 2 / image_w # Coordonée x normalisé du centre de l'objet.\n",
    "        y = (ymin + ymax) / 2 / image_h # Coordonée y normalisé du centre de l'objet.\n",
    "        w = (xmax - xmin) / image_w # Largeur normalisé\n",
    "        h = (ymax - ymin) / image_h # Longueur normalisé\n",
    "        # Récupération des coordonée de la cellule de l'objet au format 'Yolo' (la grille 7x7 en gros)\n",
    "        loc = [taillegris * x, taillegris * y] \n",
    "        loc_i = int(loc[1]) \n",
    "        loc_j = int(loc[0])\n",
    "        y = loc[1] - loc_i\n",
    "        x = loc[0] - loc_j\n",
    "\n",
    "        #Stockage des valeurs calculer dans la structure de vérité terrain\n",
    "        if label_matrix[loc_i, loc_j, 24] == 0:\n",
    "            label_matrix[loc_i, loc_j, cls] = 1\n",
    "            label_matrix[loc_i, loc_j, 20:24] = [x, y, w, h]\n",
    "            label_matrix[loc_i, loc_j, 24] = 1 \n",
    "\n",
    "    return image, label_matrix\n",
    "\n",
    "image_path = 'VOCdevkit/VOC2007/JPEGImages/000002.jpg'\n",
    "label = [\"139,200,207,301,18\"]\n",
    "im , mat = read(image_path, label)\n",
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Génération des set train/val batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "execution_failed": "2025-03-06T14:28:41.430Z",
     "iopub.execute_input": "2025-03-06T13:26:24.958579Z",
     "iopub.status.busy": "2025-03-06T13:26:24.958269Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class My_Custom_Generator(keras.utils.Sequence) :\n",
    "\n",
    "  def __init__(self, images, labels, batch_size) :\n",
    "    self.images = images\n",
    "    self.labels = labels\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "\n",
    "  def __len__(self) :\n",
    "    return int((np.ceil(len(self.images) / float(self.batch_size))))\n",
    "\n",
    "\n",
    "  def __getitem__(self, idx) :\n",
    "    batch_x = self.images[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "\n",
    "    train_image = []\n",
    "    train_label = []\n",
    "\n",
    "    for i in range(0, len(batch_x)):\n",
    "      img_path = batch_x[i]\n",
    "      label = batch_y[i]\n",
    "      image, label_matrix = read(img_path, label)\n",
    "      train_image.append(image)\n",
    "      train_label.append(label_matrix)\n",
    "    return np.array(train_image), np.array(train_label)\n",
    "\n",
    "train_datasets = []\n",
    "val_datasets = []\n",
    "\n",
    "with open(os.path.join(\"VOCdevkit\", '2007_train.txt'), 'r') as f:\n",
    "    train_datasets = train_datasets + f.readlines()\n",
    "with open(os.path.join(\"VOCdevkit\", '2007_val.txt'), 'r') as f:\n",
    "    val_datasets = val_datasets + f.readlines()\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "X_val = []\n",
    "Y_val = []\n",
    "\n",
    "for item in train_datasets:\n",
    "  item = item.replace(\"\\n\", \"\").split(\" \")\n",
    "  X_train.append(item[0])\n",
    "  arr = []\n",
    "  for i in range(1, len(item)):\n",
    "    arr.append(item[i])\n",
    "  Y_train.append(arr)\n",
    "\n",
    "for item in val_datasets:\n",
    "  item = item.replace(\"\\n\", \"\").split(\" \")\n",
    "  X_val.append(item[0])\n",
    "  arr = []\n",
    "  for i in range(1, len(item)):\n",
    "    arr.append(item[i])\n",
    "  Y_val.append(arr)\n",
    "\n",
    "batch_size = 4\n",
    "my_training_batch_generator = My_Custom_Generator(X_train, Y_train, batch_size)\n",
    "\n",
    "my_validation_batch_generator = My_Custom_Generator(X_val, Y_val, batch_size)\n",
    "\n",
    "x_train, y_train = my_training_batch_generator.__getitem__(0)\n",
    "x_val, y_val = my_training_batch_generator.__getitem__(0)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création de la derniere couche du FC, qui retourne la prédiction + proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "execution_failed": "2025-03-06T14:28:41.430Z",
     "iopub.execute_input": "2025-03-06T13:26:24.958579Z",
     "iopub.status.busy": "2025-03-06T13:26:24.958269Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Yolo_Reshape(tf.keras.layers.Layer):\n",
    "    def __init__(self, target_shape):\n",
    "        super(Yolo_Reshape, self).__init__()\n",
    "        self.target_shape = tuple(target_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'target_shape': self.target_shape\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # grids 7x7\n",
    "        S = [self.target_shape[0], self.target_shape[1]]\n",
    "        # classes\n",
    "        C = 20\n",
    "        # no of bounding boxes per grid\n",
    "        B = 2\n",
    "\n",
    "        idx1 = S[0] * S[1] * C\n",
    "        idx2 = idx1 + S[0] * S[1] * B\n",
    "\n",
    "        # Probabilité des classes \n",
    "        class_probs = tf.reshape(inputs[:, :idx1], (tf.shape(inputs)[0], S[0], S[1], C))\n",
    "        class_probs = tf.nn.softmax(class_probs)\n",
    "\n",
    "        # Score de confidance pour la classe prédite, cette partie ne seras pas utile pour plantdoc\n",
    "        confs = tf.reshape(inputs[:, idx1:idx2], (tf.shape(inputs)[0], S[0], S[1], B))\n",
    "        confs = tf.sigmoid(confs)\n",
    "\n",
    "        # Bouding boxe associé\n",
    "        boxes = tf.reshape(inputs[:, idx2:], (tf.shape(inputs)[0], S[0], S[1], B * 4))\n",
    "        boxes = tf.sigmoid(boxes)\n",
    "\n",
    "        # Concatenation des 3 ensemble\n",
    "        outputs = tf.concat([class_probs, confs, boxes], axis=-1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition de l'architecture du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "execution_failed": "2025-03-06T14:28:41.430Z",
     "iopub.execute_input": "2025-03-06T13:26:24.958579Z",
     "iopub.status.busy": "2025-03-06T13:26:24.958269Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Dropout, Flatten, Reshape\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "lrelu = tf.keras.layers.LeakyReLU(alpha=0.1)\n",
    "\n",
    "grid_w=7 \n",
    "grid_h=7\n",
    "cell_w=64\n",
    "cell_h=64\n",
    "img_w=grid_w*cell_w\n",
    "img_h=grid_h*cell_h\n",
    "\n",
    "model = Sequential() #Instanciation d'un modele sequentiel depuis tensorflow (module de classe, modele vierge)\n",
    "model.add(layers.Input(shape=(448, 448, 3))) #Couche d'entré, avec la taille de l'image\n",
    "model.add(Conv2D(filters=64, kernel_size= (7, 7), strides=(1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) #Première convolution avec 64 filtres de taille 7x7, ici on cherche les gros pattern sur l'image taille initiale\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same')) #Premire max pooling, avec une stride de 2x2, on passe donc à la deuxieme couche de yoloV1, l'image est donc a la taille 112x112\n",
    "\n",
    "model.add(Conv2D(filters=192, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) #Deuxieme convolution, on cherche donc des pattern plus fins avec 192 filtres de taille 3x3\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same')) #On redivise l'image de moitié, vers la 3éme couche du modèle, donc l'image seras la taille 56x56\n",
    "\n",
    "#On cherche des patterns encore plus fin avec 2 taille de filtre diffrent 3x3 et 1x1\n",
    "model.add(Conv2D(filters=128, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) #128 filtres de 1x1 \n",
    "model.add(Conv2D(filters=256, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) #256 filtres de 3x3\n",
    "model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) #256 Filtres de 1x1\n",
    "model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) #512 Filtres de 3x3\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same')) #On redivise l'image de moitié, vers la 4éme couche du modèle, donc l'image seras la taille 28x28\n",
    "\n",
    "#On cherche des patterns encore plus fin... on arrive donc a la couche du modèle qui repètre 4x un pattern de filtres + un bloc de 1x1 et un bloc de 3x3\n",
    "#BLOC1 (de la répétition x4)\n",
    "model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) # 256 Filtres de 1x1    \n",
    "model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) # 512 Filtres de 3x3\n",
    "#BLOC 2\n",
    "model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) # 256 Filtres de 1x1  \n",
    "model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) # 512 Filtres de 3x3\n",
    "#BLOC 3\n",
    "model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) # 256 Filtres de 1x1  \n",
    "model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) # 512 Filtres de 3x3\n",
    "#BLOC 4\n",
    "model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) # 256 Filtres de 1x1  \n",
    "model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) # 512 Filtres de 3x3\n",
    "#Et nos 512 et 1024 filtres finaux\n",
    "model.add(Conv2D(filters=512, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) #512 Filtres de 1x1\n",
    "model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) #1024 Filtres de 3x3\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same')) #On redivise l'image de moitié, vers la 5éme couche du modèle, donc l'image seras la taille 14x14\n",
    "\n",
    "#Pour l'avant derniere couche, on repète un pattern de filtre 2x pour finir avec un bloc de 1024 filtres\n",
    "#BLOC 1 (de la répétition x2)\n",
    "model.add(Conv2D(filters=512, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "#BLOC 2\n",
    "model.add(Conv2D(filters=512, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "#BLOC 3\n",
    "model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4))) #Le petit dernier bloc de 1024 Filtres de 3x3\n",
    "model.add(Conv2D(filters=1024, kernel_size= (3, 3), strides=(2, 2), padding = 'same'))  #On réalise l'équivalent d'un Max Pooling avec une convolution sans activation, on redivise donc l'image pour atteindre la taille de 7x7 (le fameux 7x7 de YOLO).\n",
    "\n",
    "#Et enfin la derniere convolution du modèle \n",
    "model.add(Conv2D(filters=1024, kernel_size= (3, 3), activation=lrelu, kernel_regularizer=l2(5e-4))) #1024 Filtres de 3x3\n",
    "model.add(Conv2D(filters=1024, kernel_size= (3, 3), activation=lrelu, kernel_regularizer=l2(5e-4))) #1024 Filtres de 3x3\n",
    "\n",
    "#LE FULLY CONNECTED !\n",
    "model.add(Flatten()) #La couche de flatten qui permet de passer 2D > 1D\n",
    "model.add(Dense(512)) \n",
    "model.add(Dense(1024))\n",
    "model.add(Dropout(0.5)) #Dropout de la moitié des neuronnes pour éviter l'overfitting..\n",
    "model.add(Dense(1470, activation='sigmoid')) # 1470 car 7*7*30 > pour nous on aura 7*7*5=245 (Je ne sais pas pourquoi dans plantdoc on avais utilisé 7x7x6 alors qu'ici on avais indiqué 7x7x5 pendant les cours...)\n",
    "model.add(Yolo_Reshape(target_shape=(7,7,30))) #Le reshape de yolo pour extraire les informations pertinentes.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du learning rate scheduler (permet de réduire dynamiquement le learning rate au fur et à mesure des épochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "execution_failed": "2025-03-06T14:28:41.430Z",
     "iopub.execute_input": "2025-03-06T13:26:24.958579Z",
     "iopub.status.busy": "2025-03-06T13:26:24.958269Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define a custom learning rate scheduler\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "class CustomLearningRateScheduler(keras.callbacks.Callback):\n",
    "    \"\"\"Learning rate scheduler which sets the learning rate according to schedule.\n",
    "\n",
    "  Arguments:\n",
    "      schedule: a function that takes an epoch index\n",
    "          (integer, indexed from 0) and current learning rate\n",
    "          as inputs and returns a new learning rate as output (float).\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, schedule):\n",
    "        super(CustomLearningRateScheduler, self).__init__()\n",
    "        self.schedule = schedule\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, \"learning_rate\"):\n",
    "            raise ValueError('Optimizer must have a \"learning_rate\" attribute.')\n",
    "        # Get the current learning rate from model's optimizer.\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        # Call schedule function to get the scheduled learning rate.\n",
    "        new_lr = self.schedule(epoch, lr)\n",
    "        # Set the value back to the optimizer before this epoch starts\n",
    "        self.model.optimizer.learning_rate.assign(new_lr)\n",
    "        print(\"\\nEpoch %05d: Learning rate is %6.4f.\" % (epoch, new_lr))\n",
    "\n",
    "\n",
    "LR_SCHEDULE = [\n",
    "    # (epoch to start, learning rate) tuples\n",
    "    (0, 0.01),\n",
    "    (75, 0.001),\n",
    "    (105, 0.0001),\n",
    "]\n",
    "\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    \"\"\"Helper function to retrieve the scheduled learning rate based on epoch.\"\"\"\n",
    "    if epoch < LR_SCHEDULE[0][0] or epoch > LR_SCHEDULE[-1][0]:\n",
    "        return lr\n",
    "    for i in range(len(LR_SCHEDULE)):\n",
    "        if epoch == LR_SCHEDULE[i][0]:\n",
    "            return LR_SCHEDULE[i][1]\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition de la fonction de perte (IoU) et quelques prétraitements des boxs afin de pouvoir calculer cette dernière."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "execution_failed": "2025-03-06T14:28:41.430Z",
     "iopub.execute_input": "2025-03-06T13:26:24.958579Z",
     "iopub.status.busy": "2025-03-06T13:26:24.958269Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "def xywh2minmax(xy, wh): # Convertis les coordonées centrés calculer plus haut en coordonées de coins (obligatoire pour le Intersection Over Union)\n",
    "    xy_min = xy - wh / 2\n",
    "    xy_max = xy + wh / 2\n",
    "\n",
    "    return xy_min, xy_max\n",
    "\n",
    "\n",
    "def iou(pred_mins, pred_maxes, true_mins, true_maxes): # Calcul de l'intersection over union entre la vérité terrain et la prédiction. (IoU est donc la métrique utilisé)\n",
    "    intersect_mins = K.maximum(pred_mins, true_mins)\n",
    "    intersect_maxes = K.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh = K.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "\n",
    "    pred_wh = pred_maxes - pred_mins\n",
    "    true_wh = true_maxes - true_mins\n",
    "    pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
    "    true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores = intersect_areas / union_areas\n",
    "\n",
    "    return iou_scores\n",
    "\n",
    "\n",
    "def yolo_head(feats): # Permet de transformer les prédictions du modèle en coordonées absolus dans l'images (On passe de format normalisé par cellule de grille (découpage en portion 7x7 de l'image) vers les coordonée sur l'image 448x448)\n",
    "    # Ici le code fournis sur le papier disposer déjà de commentaire en anglais pour expliquer certaines lignes, on les as conservé.\n",
    "    # Dynamic implementation of conv dims for fully convolutional model.\n",
    "    conv_dims = K.shape(feats)[1:3]  # assuming channels last\n",
    "    # In YOLO the height index is the inner most iteration.\n",
    "    conv_height_index = K.arange(0, stop=conv_dims[0])\n",
    "    conv_width_index = K.arange(0, stop=conv_dims[1])\n",
    "    conv_height_index = K.tile(conv_height_index, [conv_dims[1]])\n",
    "\n",
    "    # TODO: Repeat_elements and tf.split doesn't support dynamic splits.\n",
    "    # conv_width_index = K.repeat_elements(conv_width_index, conv_dims[1], axis=0)\n",
    "    conv_width_index = K.tile(\n",
    "        K.expand_dims(conv_width_index, 0), [conv_dims[0], 1])\n",
    "    conv_width_index = K.flatten(K.transpose(conv_width_index))\n",
    "    conv_index = K.transpose(K.stack([conv_height_index, conv_width_index]))\n",
    "    conv_index = K.reshape(conv_index, [1, conv_dims[0], conv_dims[1], 1, 2])\n",
    "    conv_index = K.cast(conv_index, K.dtype(feats))\n",
    "\n",
    "    conv_dims = K.cast(K.reshape(conv_dims, [1, 1, 1, 1, 2]), K.dtype(feats))\n",
    "\n",
    "    box_xy = (feats[..., :2] + conv_index) / conv_dims * 448\n",
    "    box_wh = feats[..., 2:4] * 448\n",
    "\n",
    "    return box_xy, box_wh\n",
    "\n",
    "\n",
    "def yolo_loss(y_true, y_pred): # Pareil ici on as garder les commentaire de l'auteur. \n",
    "    label_class = y_true[..., :20]  # ? * 7 * 7 * 20 >> Les classes cibles (One hot encoding).\n",
    "    label_box = y_true[..., 20:24]  # ? * 7 * 7 * 4 >> Les boites de vérités terrain (x,y,w,h) qui ont était créer plus tot.\n",
    "    response_mask = y_true[..., 24]  # ? * 7 * 7 >> Valeur binaire indiquant la présence d'un objet.\n",
    "    response_mask = K.expand_dims(response_mask, axis=-1)  # ? * 7 * 7 * 1 \n",
    "\n",
    "    #La classe, la confidence et la box prédite par le modèle\n",
    "    predict_class = y_pred[..., :20]  # ? * 7 * 7 * 20\n",
    "    predict_trust = y_pred[..., 20:22]  # ? * 7 * 7 * 2\n",
    "    predict_box = y_pred[..., 22:]  # ? * 7 * 7 * 8\n",
    "\n",
    "    _label_box = K.reshape(label_box, [-1, 7, 7, 1, 4])\n",
    "    _predict_box = K.reshape(predict_box, [-1, 7, 7, 2, 4])\n",
    "\n",
    "    #Convertion des box vérité terrain pour le calcul de l'IoU\n",
    "    label_xy, label_wh = yolo_head(_label_box)  # ? * 7 * 7 * 1 * 2, ? * 7 * 7 * 1 * 2\n",
    "    label_xy = K.expand_dims(label_xy, 3)  # ? * 7 * 7 * 1 * 1 * 2\n",
    "    label_wh = K.expand_dims(label_wh, 3)  # ? * 7 * 7 * 1 * 1 * 2\n",
    "    label_xy_min, label_xy_max = xywh2minmax(label_xy, label_wh)  # ? * 7 * 7 * 1 * 1 * 2, ? * 7 * 7 * 1 * 1 * 2\n",
    "\n",
    "    #Convertion des box prédiction pour le calcul de l'IoU\n",
    "    predict_xy, predict_wh = yolo_head(_predict_box)  # ? * 7 * 7 * 2 * 2, ? * 7 * 7 * 2 * 2\n",
    "    predict_xy = K.expand_dims(predict_xy, 4)  # ? * 7 * 7 * 2 * 1 * 2\n",
    "    predict_wh = K.expand_dims(predict_wh, 4)  # ? * 7 * 7 * 2 * 1 * 2\n",
    "    predict_xy_min, predict_xy_max = xywh2minmax(predict_xy, predict_wh)  # ? * 7 * 7 * 2 * 1 * 2, ? * 7 * 7 * 2 * 1 * 2\n",
    "\n",
    "    #calculs des IoU de chaque box prédites\n",
    "    iou_scores = iou(predict_xy_min, predict_xy_max, label_xy_min, label_xy_max)  # ? * 7 * 7 * 2 * 1\n",
    "    best_ious = K.max(iou_scores, axis=4)  # ? * 7 * 7 * 2  >> Récupére la meilleur IoU\n",
    "    best_box = K.max(best_ious, axis=3, keepdims=True)  # ? * 7 * 7 * 1 >> Récupére la meilleur box (slicing avec la meileur IoU)\n",
    "    box_mask = K.cast(best_ious >= best_box, K.dtype(best_ious))  # ? * 7 * 7 * 2  \n",
    "\n",
    "    #Calcul du score de confidence, représente la probabilité que l(objet soit présent dans la zone.\n",
    "    no_object_loss = 0.5 * (1 - box_mask * response_mask) * K.square(0 - predict_trust)\n",
    "    object_loss = box_mask * response_mask * K.square(1 - predict_trust)\n",
    "    confidence_loss = no_object_loss + object_loss\n",
    "    confidence_loss = K.sum(confidence_loss)\n",
    "\n",
    "    #Calcul de la répartition des classes dans la prédiciton, comparé a celle du vrai dataset.\n",
    "    #Cette partie ne seras pas utile pour plantdoc car on n'as qu'une seule classe.\n",
    "    class_loss = response_mask * K.square(label_class - predict_class)\n",
    "    class_loss = K.sum(class_loss)\n",
    "\n",
    "    _label_box = K.reshape(label_box, [-1, 7, 7, 1, 4])\n",
    "    _predict_box = K.reshape(predict_box, [-1, 7, 7, 2, 4])\n",
    "\n",
    "    label_xy, label_wh = yolo_head(_label_box)  # ? * 7 * 7 * 1 * 2, ? * 7 * 7 * 1 * 2\n",
    "    predict_xy, predict_wh = yolo_head(_predict_box)  # ? * 7 * 7 * 2 * 2, ? * 7 * 7 * 2 * 2\n",
    "\n",
    "    box_mask = K.expand_dims(box_mask)\n",
    "    response_mask = K.expand_dims(response_mask, axis=-1)\n",
    "\n",
    "    box_loss = 5 * box_mask * response_mask * K.square((label_xy - predict_xy) / 448)\n",
    "    box_loss += 5 * box_mask * response_mask * K.square((K.sqrt(label_wh) - K.sqrt(predict_wh)) / 448)\n",
    "    box_loss = K.sum(box_loss)\n",
    "\n",
    "    loss = confidence_loss + class_loss + box_loss\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajout d'un callback pour sauvegarde le modèle pendant l'entrainement, permet de restaurer celui avec les meilleures métriques à la fin de l'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "execution_failed": "2025-03-06T14:28:41.430Z",
     "iopub.execute_input": "2025-03-06T13:26:24.958579Z",
     "iopub.status.busy": "2025-03-06T13:26:24.958269Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# defining a function to save the weights of best model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "mcp_save = ModelCheckpoint('weight.keras', save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "execution_failed": "2025-03-06T14:28:41.430Z",
     "iopub.execute_input": "2025-03-06T13:26:24.958579Z",
     "iopub.status.busy": "2025-03-06T13:26:24.958269Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model.compile(loss=yolo_loss ,optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boucle d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.fit(x=my_training_batch_generator,\n",
    "          steps_per_epoch = int(len(X_train) // batch_size),\n",
    "          epochs = 135,\n",
    "          verbose = 1,\n",
    "          validation_data = my_validation_batch_generator,\n",
    "          validation_steps = int(len(X_val) // batch_size),\n",
    "           callbacks=[\n",
    "              CustomLearningRateScheduler(lr_schedule),\n",
    "              mcp_save\n",
    "          ])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
